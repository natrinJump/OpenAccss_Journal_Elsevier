{"cells":[{"cell_type":"markdown","id":"15a4ddb9","metadata":{"id":"15a4ddb9"},"source":["Result:\n","\n","<table>\n","    <thead>\n","        <tr>\n","            <th>Weight type</th>\n","            <th colspan = 3>Overall Accuracy</th>\n","            <th colspan = 5>Accuracy by subject size</th>\n","        </tr>\n","        <tr>\n","            <th/>\n","            <th>Top 1</th>\n","            <th>Top 3</th>\n","            <th>Top 5</th>\n","            <th>Subject size 1</th>\n","            <th>Subject size 2</th>\n","            <th>Subject size 3</th>\n","            <th>Subject size 4</th>\n","            <th>Subject size 5</th>\n","        </tr>\n","    </thead>\n","    <tbody>\n","        <tr>\n","            <td>Dynamic weight</td>\n","            <td>76.8%</td>\n","            <td>93.5%</td>\n","            <td>96.9%</td>\n","            <td>67.8%</td>\n","            <td>82.1%</td>\n","            <td>84.2%</td>\n","            <td>95.3%</td>\n","            <td>100%</td>\n","        </tr>\n","        <tr>\n","            <td>100% subject area</td>\n","            <td>59.7%</td>\n","            <td>82.4%</td>\n","            <td>90.4%</td>\n","            <td>47.7%</td>\n","            <td>63.4%</td>\n","            <td>71.9%</td>\n","            <td>97.4%</td>\n","            <td>100%</td>\n","        </tr>\n","        <tr>\n","            <td>25% abstract / 75% subject area</td>\n","            <td>73.5%</td>\n","            <td>93.7%</td>\n","            <td>96.7%</td>\n","            <td>63.4%</td>\n","            <td>77.%</td>\n","            <td>84.4%</td>\n","            <td>97.9%</td>\n","            <td>100%</td>\n","        </tr>\n","        <tr>\n","            <td>50% abstract / 50% subject area</td>\n","            <td>79.1%</td>\n","            <td>93.8%</td>\n","            <td>96.8%</td>\n","            <td>67.6%</td>\n","            <td>85.0%</td>\n","            <td>91.9%</td>\n","            <td>100%</td>\n","            <td>100%</td>\n","        </tr>\n","        <tr>\n","            <td>75% abstract / 25% subject area</td>\n","            <td>74.8%</td>\n","            <td>93.3%</td>\n","            <td>96.9%</td>\n","            <td>65.2%</td>\n","            <td>79.1%</td>\n","            <td>84.4%</td>\n","            <td>98.5%</td>\n","            <td>100%</td>\n","        </tr>\n","        <tr>\n","            <td>100% abstract</td>\n","            <td>69.7%</td>\n","            <td>87.8%</td>\n","            <td>93.8%</td>\n","            <td>64.4%</td>\n","            <td>71.4%</td>\n","            <td>73.9%</td>\n","            <td>89.2%</td>\n","            <td>100%</td>\n","        </tr>\n","    </tbody>\n","</table>\n","\n","\n"]},{"cell_type":"markdown","id":"625b6005","metadata":{"id":"625b6005"},"source":["Import"]},{"cell_type":"code","execution_count":null,"id":"03101d99","metadata":{"id":"03101d99","outputId":"b389f445-fa8f-4802-ccfd-ef3d99726580"},"outputs":[{"name":"stderr","output_type":"stream","text":["D:\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import pickle\n","import scipy\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from keras.utils import to_categorical\n","from keras.models import load_model\n","\n","import numpy as np\n","import os\n","\n","# Hugging Face Transformers (SciBERT)\n","from transformers import AutoModel, AutoTokenizer\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from tqdm import tqdm\n","\n","#panda\n","import pandas as pd\n","\n","# Text Processing\n","import re\n","import nltk\n","from nltk import word_tokenize, download\n","from nltk.corpus import stopwords\n","\n","# Data Serialization and Deserialization\n","import ast\n","\n","from collections import Counter\n","import tensorflow as tf\n","\n","\n","sciBert_model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"]},{"cell_type":"code","execution_count":null,"id":"58b0ff12","metadata":{"id":"58b0ff12"},"outputs":[],"source":["subjs_model_acc = {\n","    'deci': 0.8226,\n","    'mult': 0.8685,\n","    'eart': 0.5340,\n","    'mate': 0.4698,\n","    'econ': 0.6328,\n","    'envi': 0.4267,\n","    'agri': 0.6570,\n","    'phar': 0.5635,\n","    'neur': 0.5363,\n","    'comp': 0.8170,\n","    'nurs': 1,\n","    'medi': 0.7262,\n","    'soci': 0.5093,\n","    'immu': 0.7276,\n","    'arts': 0.8276,\n","    'chem': 0.5857,\n","    'busi': 0.7099,\n","    'math': 0.9808,\n","    'phys': 0.6064,\n","    'ceng': 0.6037,\n","    'heal': 0.9884,\n","    'bioc': 0.6193,\n","    'psyc': 0.6032,\n","    'ener': 0.4237,\n","    'engi': 0.4870,\n","    'vete': 0.9663,\n","}"]},{"cell_type":"code","execution_count":null,"id":"820c70eb","metadata":{"id":"820c70eb"},"outputs":[],"source":["subj_acc = 0.579"]},{"cell_type":"markdown","id":"6addb626","metadata":{"id":"6addb626"},"source":["Label encoder"]},{"cell_type":"code","execution_count":null,"id":"61203691","metadata":{"id":"61203691"},"outputs":[],"source":["# Change the file path if needed\n","le_filename = 'models/labelencoder.pkl'\n","# load the model from disk\n","with open(le_filename, 'rb') as f:\n","    le = pickle.load(f)"]},{"cell_type":"markdown","id":"5e4ae4f5","metadata":{"id":"5e4ae4f5"},"source":["Load subject model"]},{"cell_type":"code","execution_count":null,"id":"5489ca6d","metadata":{"id":"5489ca6d","outputId":"97d2b950-187f-4d04-dc82-7ba186d49641"},"outputs":[{"name":"stderr","output_type":"stream","text":["D:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 1.1.3 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}],"source":["# Change the file path if needed\n","subject_model = load_model('models/Subject area model/subjArea.h5')\n","\n","# Change the file path if needed\n","subject_vect_filename = 'models/Subject area model/subj_vectorizer.pkl'\n","# load the model from disk\n","with open(subject_vect_filename, 'rb') as f:\n","    subj_vectorizer = pickle.load(f)"]},{"cell_type":"markdown","id":"72995b3c","metadata":{"id":"72995b3c"},"source":["Subject model prediction method"]},{"cell_type":"code","execution_count":null,"id":"527971e0","metadata":{"id":"527971e0"},"outputs":[],"source":["def to_lower(X):\n","    X = [x.lower() for x in X]\n","    return X\n","\n","def prepared_subject(subjectarea):\n","    subjectarea = to_lower(subjectarea)\n","    subjectarea = ' '.join(subjectarea)\n","    return subj_vectorizer.transform([subjectarea]).toarray()\n","\n","def subject_predict(subjectarea):\n","    subjectarea = prepared_subject(subjectarea)\n","    prediction = subject_model.predict(subjectarea, verbose=None)\n","    return prediction[0]"]},{"cell_type":"markdown","id":"e4355d69","metadata":{"id":"e4355d69"},"source":["Load abstract model"]},{"cell_type":"code","execution_count":null,"id":"164e431d","metadata":{"id":"164e431d"},"outputs":[],"source":["abstract_model = {}\n","# Change the file path if needed\n","directory_path = 'models/abstract model (scibert)/'\n","\n","file_list = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f))]\n","\n","for file in file_list:\n","    abstract_model[file[:4].lower()] = load_model(directory_path + file)"]},{"cell_type":"markdown","id":"055c5a9f","metadata":{"id":"055c5a9f"},"source":["Abstract model method"]},{"cell_type":"code","execution_count":null,"id":"0420ae16","metadata":{"id":"0420ae16"},"outputs":[],"source":["def get_embeddings(sentences, batch_size=16, max_length=500):\n","    with torch.no_grad():\n","        embeddings = []  # Initialize a list to accumulate embeddings\n","        for idx in range(0, len(sentences), batch_size):\n","            batched_sentences = sentences[idx : min(len(sentences), idx + batch_size)]\n","            encoded = tokenizer(batched_sentences, truncation=True, return_tensors='pt', padding=\"max_length\", max_length=max_length)\n","            batch_embeddings = sciBert_model(**encoded).last_hidden_state[:, 0].cpu().numpy()\n","            embeddings.extend(batch_embeddings)  # Accumulate embeddings\n","        return np.array(embeddings)\n","\n","\n","def abstract_predict(subj, abstract):\n","    model = abstract_model[subj]\n","\n","    embedded_abstract = list(get_embeddings([abstract]))\n","\n","    return model.predict(np.array(embedded_abstract), verbose=None)[0]"]},{"cell_type":"code","execution_count":null,"id":"cf2da7a0","metadata":{"id":"cf2da7a0"},"outputs":[],"source":["def dynamic_ranking(abstract, subjectarea):\n","    total_percentage = 0\n","    total_percentage += subj_acc\n","    for subject in subjectarea:\n","        total_percentage += subjs_model_acc[subject]\n","\n","    subj_pred = subject_predict(subjectarea) * (subj_acc/total_percentage)\n","\n","    total_abstract_pred = np.zeros(len(le.classes_))\n","    for subject in subjectarea:\n","        total_abstract_pred += (abstract_predict(subject, abstract) * subjs_model_acc[subject]/total_percentage)\n","\n","    pred = total_abstract_pred + subj_pred\n","    return pred\n","\n","\n","def static_ranking(abstract, subjectarea, abstract_weight=0.75, subj_weight=0.25):\n","    subj_pred = subject_predict(subjectarea)\n","\n","    total_abstract_pred = np.zeros(len(le.classes_))\n","    for subject in subjectarea:\n","        total_abstract_pred += abstract_predict(subject, abstract)\n","\n","    pred = ((total_abstract_pred / len(subjectarea)) * abstract_weight) + (subj_pred * subj_weight)\n","    return pred\n","\n","\n","def ranking(abstract, subjectarea):\n","    # pred = dynamic_ranking(abstract, subjectarea)\n","    pred = static_ranking(abstract, subjectarea, 1, 0)\n","\n","    return pred"]},{"cell_type":"markdown","id":"674ed918","metadata":{"id":"674ed918"},"source":["Load data"]},{"cell_type":"code","execution_count":null,"id":"a37480d8","metadata":{"id":"a37480d8"},"outputs":[],"source":["# Change the file path if needed\n","data = pd.read_csv(os.getcwd() + '/data/body_extracted.csv', index_col=0)\n","def extract_journal(doi):\n","    # just get text after 'j.' until another '.'\n","    match = re.search(r'j\\.([^\\.]+)', doi)\n","    if match:\n","        name = match.group(1)\n","        return name\n","\n","data['doi'] = data['doi'].apply(extract_journal)\n","\n","threshold = 22/len(data) # the remaining journals need to at least contain 22 articles\n","counts = data['doi'].value_counts(normalize=True)\n","data = data.loc[data['doi'].isin(counts[counts > threshold].index), :]"]},{"cell_type":"markdown","id":"33b9eac4","metadata":{"id":"33b9eac4"},"source":["Test accuracy"]},{"cell_type":"code","execution_count":null,"id":"8dfd68e7","metadata":{"id":"8dfd68e7","outputId":"ca36da54-af6d-4f9e-9037-3464c1479464"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|                                                                               | 2/3000 [00:06<2:28:10,  2.97s/it]"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CA80A57280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stderr","output_type":"stream","text":["  0%|▏                                                                              | 5/3000 [00:11<1:32:20,  1.85s/it]"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001CA808D6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████| 3000/3000 [3:50:53<00:00,  4.62s/it]\n"]}],"source":["prediction_ranking_list = []\n","\n","subject_size = []\n","\n","test_size = 3000\n","test_data = data.sample(n=test_size, random_state=24)\n","\n","for index, row in tqdm(test_data.iterrows(), total=test_data.shape[0]):\n","    subjareas = ast.literal_eval(row['subjareas'])\n","    subjareas = [x.lower() for x in subjareas]\n","    pred = ranking(row['abstract'], subjareas)\n","\n","    pred_index = pred.argsort()[::-1]\n","\n","    true_index = np.where(le.classes_ == row.doi)[0][0]\n","\n","    predict_ranking = np.where(pred_index==true_index)[0][0]\n","\n","    prediction_ranking_list.append(predict_ranking + 1)\n","\n","    subject_size.append(len(subjareas))"]},{"cell_type":"code","execution_count":null,"id":"d3438635","metadata":{"id":"d3438635","outputId":"3d2ee6d2-1700-4740-a350-24d9c6d5f3f4"},"outputs":[{"data":{"text/plain":["0.6973333333333334"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["top_1_acc = sum([1 if i <= 1 else 0 for i in prediction_ranking_list])/len(prediction_ranking_list)\n","top_1_acc"]},{"cell_type":"code","execution_count":null,"id":"a475ea57","metadata":{"id":"a475ea57","outputId":"b6901b7e-abd9-4d32-c828-7d5389f6439e"},"outputs":[{"data":{"text/plain":["0.8776666666666667"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["top_3_acc = sum([1 if i <= 3 else 0 for i in prediction_ranking_list])/len(prediction_ranking_list)\n","top_3_acc"]},{"cell_type":"code","execution_count":null,"id":"678e1dd5","metadata":{"id":"678e1dd5","outputId":"9c917c27-79b2-4fff-eb48-8deacd9b6a1b"},"outputs":[{"data":{"text/plain":["0.938"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["top_5_acc = sum([1 if i <= 5 else 0 for i in prediction_ranking_list])/len(prediction_ranking_list)\n","top_5_acc"]},{"cell_type":"code","execution_count":null,"id":"ad734be3","metadata":{"id":"ad734be3","outputId":"4f9aed82-7cb4-4a06-876c-9e18769cc364"},"outputs":[{"data":{"text/plain":["0.6435495898583147"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["correct_list = [prediction_ranking_list[i] for i, size in enumerate(subject_size) if size==1]\n","acc_with_sub_size_1 = sum([1 if i <= 1 else 0 for i in correct_list])/len(correct_list)\n","acc_with_sub_size_1\n"]},{"cell_type":"code","execution_count":null,"id":"47472e6a","metadata":{"id":"47472e6a","outputId":"45ab121c-2b21-475f-da9b-122aecb699d9"},"outputs":[{"data":{"text/plain":["0.7138939670932358"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["correct_list = [prediction_ranking_list[i] for i, size in enumerate(subject_size) if size==2]\n","acc_with_sub_size_2 = sum([1 if i <= 1 else 0 for i in correct_list])/len(correct_list)\n","acc_with_sub_size_2"]},{"cell_type":"code","execution_count":null,"id":"7750e054","metadata":{"id":"7750e054","outputId":"ea84462c-5a2c-4678-90ec-dcb5af5bbe85"},"outputs":[{"data":{"text/plain":["0.7388888888888889"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["correct_list = [prediction_ranking_list[i] for i, size in enumerate(subject_size) if size==3]\n","acc_with_sub_size_3 = sum([1 if i <= 1 else 0 for i in correct_list])/len(correct_list)\n","acc_with_sub_size_3"]},{"cell_type":"code","execution_count":null,"id":"8a5ccd27","metadata":{"id":"8a5ccd27","outputId":"17ccf1c6-a947-44ca-b0c1-d26a9e77e4b9"},"outputs":[{"data":{"text/plain":["0.8923076923076924"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["correct_list = [prediction_ranking_list[i] for i, size in enumerate(subject_size) if size==4]\n","acc_with_sub_size_4 = sum([1 if i <= 1 else 0 for i in correct_list])/len(correct_list)\n","acc_with_sub_size_4"]},{"cell_type":"code","execution_count":null,"id":"30ea30d8","metadata":{"id":"30ea30d8","outputId":"6aa80b49-bdb5-44ee-e564-161f4f05815e"},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["correct_list = [prediction_ranking_list[i] for i, size in enumerate(subject_size) if size==5]\n","acc_with_sub_size_5 = sum([1 if i <= 1 else 0 for i in correct_list])/len(correct_list)\n","acc_with_sub_size_5"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}